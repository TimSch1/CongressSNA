---
title: '113th Congress Network Analysis: Does Cooperativity influence Electability?'
author: "Timothy J. Schmeier, PhD"
date: "Monday, February 09, 2015"
output: html_document
---
This analysis explores cooperativity among members of the House of Representatives during the 113th Congress and its subsequent influence on the elections in 2014. This analysis considers cooperativity as working across party lines to co-sponsor successful, relevant legislation. I have defined relevant legislation as both temporally pertinent and publicly impactful. As such, legislation renaming public spaces as well as posthumous awards of valor were excluded from this analysis. The primary methodology used to investigate this question is Social Network Analysis (SNA) which is the study of relationships between actors. The terminology and concepts of SNA relevant to this discussion will be described briefly throughout. 

###Data Preparation

```{r, warning=FALSE, message=FALSE, eval=TRUE, echo=FALSE}
Congress = read.csv('C:/Users/TimBo/Downloads/R docs and scripts/CongressSNA/113thCongress.csv', header = TRUE)
Congress = Congress[,-c(13,14,18,20,23,24,26,33,51,60,62,77)]
Congress[is.na(Congress)] = 0
```

This code separates the House from the Senate, distinguishes the two Mike Rogers in the House, and makes the character strings uniform.

```{r, warning=FALSE, message=FALSE}
library(stringr)
Reps = str_detect(Congress$Senator.Rep, 'Rep')
Congress = Congress[Reps,]
Congress$Senator.Rep = gsub('Rep', '', Congress$Senator.Rep)
Congress$Senator.Rep = str_trim(Congress$Senator.Rep, side='both')
grep('Rogers, Mike', Congress$Senator.Rep)
Congress$Senator.Rep[233] = 'Rogers1, Mike'
```

The following code creates an edge list, defined as successful co-sponsored legislation, between members of the same party and members of the opposite party to be used as features for a later model.

```{r, warning=FALSE, message=FALSE}
library(plyr)
library(dplyr)
DHouse = filter(Congress, Party == 'D')
RHouse = filter(Congress, Party == 'R')
rownames(RHouse) = RHouse$Senator.Rep
rownames(DHouse) = DHouse$Senator.Rep
DHouse = DHouse[,-(1:2)]
RHouse = RHouse[,-(1:2)]
DemsWithin = rowSums(as.matrix(DHouse) %*%t(as.matrix(DHouse)))
RepsWithin = rowSums(as.matrix(RHouse) %*%t(as.matrix(RHouse)))
Coopertivity = rowSums(as.matrix(DHouse) %*%t(as.matrix(RHouse)))
Coopertivity2 = rowSums(as.matrix(RHouse) %*%t(as.matrix(DHouse)))
WithinParty = c(RepsWithin, DemsWithin)
AcrossAisle = c(Coopertivity, Coopertivity2)
Links = merge(WithinParty,AcrossAisle,by=0)
colnames(Links) = c('Name','Within','Coopertivity')
```

###Social Network Analysis

One primary question in SNA is that of centrality; how important or how much influence does an actor wield within their community? Betweenness is defined as the number of shortest paths between any two community members an actor is in. Another common measure of centrality is Eigenvector Centrality, which weights links to important actors more than links to unimportant actors (similar to the Google page-rank algorithm). These metrics are approximately linear, so when Eigenvector Centrality is regressed on Betweenness outliers may reveal interesting actors. An actor with a high degree of betweenness but a large negative residual (low Eigenvector Centrality) suggests a "gate-keeper" or "bridge", an actor controlling access to leadership or controlling access between two separate communities. In contrast, an actor with a large positive residual (large Eigenvector Centrality) relative to betweenness indicates a "pulse-taker" an actor with unique access to leadership and a potential successor to leadership positions.

```{r, warning=FALSE, message=FALSE}
rownames(Congress) = Congress$Senator.Rep
Party = Congress$Party
Congress = Congress[,-(1:2)]
CongressAdj = as.matrix(Congress) %*% t(as.matrix(Congress))

library(igraph)
g = graph.adjacency(CongressAdj, mode = 'undirected', weighted = TRUE)
g = simplify(g)
HouseCent = data.frame(bet = betweenness(g), eig = evcent(g)$vector)
HouseCent = transform(HouseCent, res = lm(eig~bet, data= HouseCent)$residuals)

library(ggplot2)
Actors = ggplot(HouseCent, (aes(x=bet, y=eig, label = rownames(HouseCent), 
                      color=Party, size = abs(res)))) + xlab('Betweenness Centrality')+
                      ylab('Eigenvector Centrality')+theme_bw()

Actors+scale_color_manual(values=c('blue','red')) + geom_text() + scale_size("Residuals")+
      labs(title = 'Eigenvector and Betweenness Centrality')
```

As seen in the Figure Peter King and Ben Ray Lujan appear to be "gate-keepers" for the Republican and Democratic Parties respectively while Pat Meehan, Walter Jones, Pete Olson are the legislative leadership group with the largest number of important connections. The "pulse-takers" or potential successors appear beneath the leadership group. Next the 113th congress was visualized with a network graph.

```{r, warning=FALSE, message=FALSE}
V(g)$color = ifelse(Party == 'D', 'blue', 'red')
nodes = V(g)$name
V(g)$label.cex = 0.4
V(g)$label.color='black'
layout1 = layout.fruchterman.reingold(g, niter=500)
set.seed(200)
plot.new()
plot(g, layout1,  vertex.label = nodes, vertex.size = HouseCent$bet^(1/3), 
     edge.width=E(g)$weight)
```

It is difficult to see anything from this plot, resolution is lost because of the 3 outliers. However it is worth noting the connection from Eleanor Norton to Darrell Issa. As a non-voting delegate from Washington D.C. Eleanor Norton may be introducing legislation through a proxy. To more clearly visualize the network structure, the graph was replotted focusing on the core network; only actors with over 50 connections are included. The vertex size is plotted as a function of Betweenness and only names of the most important congress members are included.  

```{r, warning=FALSE, message=FALSE}
cores = graph.coreness(g)
g2 = induced.subgraph(g, as.vector(which(cores>50)))
nodes = as.vector(V(g2))
nodes = V(g2)$name
V(g2)$label.cex=0.5
nodes[which(HouseCent$eig<.5)] = NA
layout2 = layout.fruchterman.reingold(g2, niter=500)
plot(g2, layout = layout2, vertex.label=nodes, vertex.size = HouseCent$bet^(1/3), vertex.label.dist = 0.25, vertex.label.color = 'black')
```

Unsurprisingly, the largest vertexes and the majority of important congress members are in the center of the plot. More interesting are the congress members connecting outside congressional members (not shown, fewer than 50 links) to the inner core thus possessing a large betweenness and (relative to their position) an enhanced Eigenvector centrality. Also remarkable is the structure apparent in the network, there appear to be a number of clusters, some dominated by democrats, others by republicans, and a few cooperative clusters which have similar numbers of members from each party. This observation prompted a clustering analyses. A hierarchical clustering and a walk-trap community analysis (an algorithm exploiting the property that random walks of limited length have a high probability of remaining within the same community) were conducted next in order to investigate the network structure further.

```{r, warning=FALSE, message=FALSE}
library(sna)
library(ggdendro)
set.seed(200)
Clust = equiv.clust(CongressAdj, method='hamming', cluster.method='ward.D')
hClustAssignments = cutree(Clust$cluster, k=6)
den = as.dendrogram(Clust$cluster)
ddata = dendro_data(den, type='rectangle')
ddata$labels[,3] = rownames(CongressAdj)
ddata$labels = cbind(ddata$labels, Party)

ggplot() +
       geom_segment(data=segment(ddata), aes(x=x, y=y, xend=xend, yend=yend)) +
       geom_text(data = label(ddata), aes(x=x, y=y, label=label, 
                                          colour = Party, hjust=0),   size=1) +
       geom_point(data = label(ddata), aes(x=x, y=y), size=3, shape = 21) +
       coord_flip() +
       scale_y_reverse(expand=c(0.2, 0)) +
       scale_colour_manual(values=c('blue','red')) + 
       theme_dendro()

set.seed(200)
wc = walktrap.community(g)
walktrapComm = wc$membership
plot(wc, g)
```

Removing the outliers and vertex names to reveal the underlying structure provides the following plot.

```{r, echo=FALSE}
detach('package:sna', unload=T)
library(igraph)
Congress2 = Congress[-c(49,343,349),]
CongressAdj2 = as.matrix(Congress2) %*% t(as.matrix(Congress2))
g3 = graph.adjacency(CongressAdj2, mode = 'undirected', weighted = TRUE)
g3 = simplify(g3)
V(g3)$label = NA
wc2 = walktrap.community(g3)
plot(wc2, g3)
```

Both clustering algorithms identified clusters of partisan and cooperative congress members although the walk-trap algorithm identified a larger number of small clusters. The composition of each cluster by party membership is shown below.

```{r}
table(Party, wc$membership)
table(Party, hClustAssignments)
```

A covariate, the number of previous terms, as well as the results of the 2014 election were scraped from Wikipedia's website with the following code and all the features were merged for modeling.


```{r}
library(rvest)

wiki = html('http://en.wikipedia.org/wiki/United_States_House_of_Representatives_elections,_2014')

Elect = wiki %>% html_nodes('td a:nth-child(1)') %>% html_text()

Elect = Elect[212:1530]
Elect = Elect[-c(45,58,65,171,238,350,378,382,395,489,547,554,648,712,741,892,899,968,1023,1204)]

vect = rep(1:3, length(Elect)/3)
Name = Elect[vect==1]; Terms = Elect[vect==2]; Winner = Elect[vect==3]
Elect = data.frame(Name, Terms, Winner)
Elect$Winner = ifelse(Elect$Name==Winner, 1, 0) 
Elect$Terms = cut(as.numeric(as.character(Elect$Terms)), 
                  seq(from = 2014, to = 1954, by = -2))
levels(Elect$Terms) = seq(from = 30, to = 1, by=-1)

Elect$Name = sapply(Elect$Name, function(x) strsplit(as.character(x), split=' '))
Elect$Name = sapply(Elect$Name, rev)
Elect$Name = unlist(lapply(Elect$Name, paste, collapse = ', '))

Cooperativity = data.frame(Name = rownames(CongressAdj), Party = Party, 
                          hClustAssign = hClustAssignments, wtComm = walktrapComm)
Cooperativity = merge(Cooperativity, Links, by='Name')
Cooperativity = merge(Cooperativity, Elect, by='Name')
head(Cooperativity)
```

###Predictive Modeling

After splitting the data into training and test sets, a number of classifiers were trained. The 17% event rate (not re-elected) made resampling positive events a necessity. In this case the null model would predict re-election in all cases and be 83% accurate but miss all the events of interest. The model parameters chosen maximized the area under the ROC curve in an attempt to increase sensitivity and capture some of the events of interest.

The predict_plot function returns a list of summary statistics from the predicted test set values. The function also plots the ROC curve of the values predicted by the model for the test set.

```{r, warning=FALSE, message=FALSE}
library(caret)
Cooperativity$Winner = factor(Cooperativity$Winner, levels=c(0,1), labels=c('Lost','Won'))

train = createDataPartition(Cooperativity$Winner, list=F, p=.9)
training = Cooperativity[train,]
test = Cooperativity[-train,]

upSampled = upSample(training[,-1], training$Winner)
upSampled = upSampled[,-8]
test = test[,-1]

fitControl = trainControl(method = 'repeatedcv', number=10, repeats=3, classProbs=TRUE, summaryFunction=twoClassSummary)

tuneGrid = expand.grid(interaction.depth = c(1:4), n.trees = seq(1,500,50), shrinkage = 0.1)
gbm.Mod = train(Winner~., data=upSampled, method = 'gbm', trControl=fitControl, verbose=FALSE, tuneGrid=tuneGrid, metric='ROC')

grid = expand.grid(.model='tree', .trials = c(1:100), .winnow=FALSE)
C5.Mod = train(Winner~., data=upSampled, method = 'C5.0', trControl=fitControl, tuneGrid=grid, metric='ROC')

svm.Mod = train(Winner~., data=upSampled, method = 'svmRadial', trControl=fitControl, tuneLength=9, preProc =c('center','scale'), metric='ROC')

model.list = list(gbm.Mod, C5.Mod, svm.Mod)

set.seed(100)
predict_plot = function(x){
  models.predict = lapply(model.list, predict, newdata=test, type='prob')
  models.predict2 = lapply(model.list, predict, newdata=test)
  stats = lapply(models.predict2, function(x) confusionMatrix(x, test$Winner))
  model.ROC = lapply(models.predict, function(x) roc(test$Winner, x[,'Lost'], levels=rev(levels(test$Winner))))
  color = colors()[(round(runif(length(x))*1000))]
  for(i in 1:length(model.ROC)){
    if(i==1) {
      plot(model.ROC[[i]], col=color[[i]], legacy.axes=T)
    } else {
      plot(model.ROC[[i]], col= color[[i]], add=T)
    }
  }
  legend('bottomright', legend=c('GBM','C5.0','SVM'), col=color, lty=1)
  return(stats)
}

predict_plot(model.list)
```

###Conclusions

Regrettably, the sensitivity of all models was poor as none of them were able to reliably predict the event of interest. This indicated that none of the features in this dataset are useful predictors when modeling election outcomes. This analysis may have revealed what politicians already know; voters won't punish them for lackluster legislative activity or lack of cross party cooperativity. 
